from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import spacy
from thefuzz import fuzz
import uvicorn
from typing import Optional
from sentence_transformers import SentenceTransformer
import torch
import numpy as np

app = FastAPI(title="Embed Relevance Analyzer", version="1.0.0")

# Load semantic model (multilingual)
print("Loading semantic model...")
semantic_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
print("Semantic model loaded!")

# Load spaCy model
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    # If model not found, download it
    import subprocess
    subprocess.run(["python", "-m", "spacy", "download", "en_core_web_sm"])
    nlp = spacy.load("en_core_web_sm")

class RelevanceRequest(BaseModel):
    articleTitle: str
    articleSummary: str
    embedType: str
    embedContent: str
    articleText: str

class RelevanceResponse(BaseModel):
    keep: bool
    reason: str
    scores: Optional[dict] = None

@app.post("/analyze-relevance", response_model=RelevanceResponse)
async def analyze_relevance(request: RelevanceRequest):
    """
    Analyze if an embed is relevant to an article using semantic understanding.
    """
    try:
        # Create comprehensive article text
        article_text = f"{request.articleTitle}. {request.articleSummary}. {request.articleText}"

        # Semantic similarity using sentence transformers (multilingual)
        article_embedding = semantic_model.encode(article_text, convert_to_tensor=True)
        embed_embedding = semantic_model.encode(request.embedContent, convert_to_tensor=True)

        # Calculate cosine similarity between embeddings
        semantic_sim = torch.cosine_similarity(article_embedding, embed_embedding, dim=0).item()

        # Traditional TF-IDF as backup (for very short texts)
        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
        texts = [article_text, request.embedContent]
        vectors = vectorizer.fit_transform(texts)
        tfidf_sim = float(cosine_similarity(vectors[0:1], vectors[1:2])[0][0])

        # Named entity overlap (important for news articles)
        article_doc = nlp(article_text)
        embed_doc = nlp(request.embedContent)

        article_entities = set([ent.text.lower() for ent in article_doc.ents])
        embed_entities = set([ent.text.lower() for ent in embed_doc.ents])

        entity_overlap = len(article_entities.intersection(embed_entities)) / len(article_entities.union(embed_entities)) if article_entities.union(embed_entities) else 0

        # Smart content-based relevance scoring (no hardcoded keywords)
        # Analyze how well the embed content aligns with and references the article

        # 1. Cross-reference boost: Does the embed appear to reference the article?
        cross_reference_boost = 0

        embed_lower = request.embedContent.lower()

        # Check if embed mentions key entities from the article
        key_article_entities = set()
        for ent in article_doc.ents:
            if ent.label_ in ['ORG', 'PERSON', 'GPE', 'MONEY', 'PERCENT']:
                key_article_entities.add(ent.text.lower())

        embed_entity_mentions = sum(1 for entity in key_article_entities
                                  if entity in embed_lower)
        if embed_entity_mentions > 0:
            cross_reference_boost += min(0.15, embed_entity_mentions * 0.05)  # Cap at 0.15

        # 2. Authority language patterns (using semantic similarity to authoritative content)
        # Compare embed against patterns of authoritative communication
        authority_patterns = [
            "official statement",
            "according to sources",
            "confirmed by",
            "announced today",
            "press release",
            "spokesperson said"
        ]

        authority_embeddings = semantic_model.encode(authority_patterns, convert_to_tensor=True)
        embed_auth_similarity = torch.mean(torch.cosine_similarity(
            embed_embedding.unsqueeze(0), authority_embeddings, dim=1
        )).item()

        authority_boost = embed_auth_similarity * 0.1  # Scale down the authority signal

        # 3. Topic coherence: Are they discussing the same subject?
        # Use sentence-level analysis for better topic detection
        article_sentences = [sent.text.strip() for sent in article_doc.sents if len(sent.text.strip()) > 10]
        embed_sentences = [sent.text.strip() for sent in embed_doc.sents if len(sent.text.strip()) > 10]

        if article_sentences and embed_sentences:
            # Compare sentence embeddings for topic coherence
            article_sent_embeddings = semantic_model.encode(article_sentences[:3], convert_to_tensor=True)  # First 3 sentences
            embed_sent_embeddings = semantic_model.encode(embed_sentences[:2], convert_to_tensor=True)    # First 2 sentences

            topic_coherence = torch.mean(torch.cosine_similarity(
                article_sent_embeddings.unsqueeze(1),
                embed_sent_embeddings.unsqueeze(0),
                dim=2
            )).item()

            topic_boost = topic_coherence * 0.1
        else:
            topic_boost = 0

        # Combine intelligent boosts
        content_boost = cross_reference_boost + authority_boost + topic_boost

        # Enhanced scoring algorithm
        # Semantic similarity is now the primary factor (60%)
        # Entity overlap catches important names/places (25%)
        # Content boost uses AI to assess relevance without hardcoded keywords (15%)
        final_score = (semantic_sim * 0.6) + (entity_overlap * 0.25) + content_boost

        # Adaptive threshold based on content length
        if len(request.embedContent) < 50:  # Very short embeds
            threshold = 0.3
        elif len(request.embedContent) < 150:  # Medium embeds
            threshold = 0.25
        else:  # Long embeds
            threshold = 0.2

        keep = final_score > threshold

        # Generate meaningful reason
        if semantic_sim > 0.6:
            semantic_reason = "Strong semantic similarity"
        elif semantic_sim > 0.3:
            semantic_reason = "Good semantic similarity"
        else:
            semantic_reason = "Low semantic similarity"

        if entity_overlap > 0.3:
            entity_reason = "high entity overlap"
        elif entity_overlap > 0.1:
            entity_reason = "some entity overlap"
        else:
            entity_reason = "low entity overlap"

        boost_reason = f"content boost: {content_boost:.2f}" if content_boost > 0 else "no content boost"

        reason = f"{semantic_reason}, {entity_reason}, {boost_reason}"

        scores = {
            "semantic_similarity": round(semantic_sim, 3),
            "entity_overlap": round(entity_overlap, 3),
            "content_boost": round(content_boost, 3),
            "final_score": round(final_score, 3),
            "threshold": round(threshold, 3)
        }

        return RelevanceResponse(keep=keep, reason=reason, scores=scores)

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy"}

@app.post("/explain-relevance")
async def explain_relevance(request: RelevanceRequest):
    """
    Provide detailed explanation of embed relevance analysis using semantic understanding.
    """
    try:
        # Create comprehensive article text
        article_text = f"{request.articleTitle}. {request.articleSummary}. {request.articleText}"

        # Semantic similarity using sentence transformers (multilingual)
        article_embedding = semantic_model.encode(article_text, convert_to_tensor=True)
        embed_embedding = semantic_model.encode(request.embedContent, convert_to_tensor=True)

        # Calculate cosine similarity between embeddings
        semantic_sim = torch.cosine_similarity(article_embedding, embed_embedding, dim=0).item()

        # Named entity overlap (important for news articles)
        article_doc = nlp(article_text)
        embed_doc = nlp(request.embedContent)

        article_entities = set([ent.text.lower() for ent in article_doc.ents])
        embed_entities = set([ent.text.lower() for ent in embed_doc.ents])

        entity_overlap = len(article_entities.intersection(embed_entities)) / len(article_entities.union(embed_entities)) if article_entities.union(embed_entities) else 0

        # Smart content-based relevance scoring (no hardcoded keywords)
        # Analyze how well the embed content aligns with and references the article

        # 1. Cross-reference boost: Does the embed appear to reference the article?
        cross_reference_boost = 0

        embed_lower = request.embedContent.lower()

        # Check if embed mentions key entities from the article
        key_article_entities = set()
        for ent in article_doc.ents:
            if ent.label_ in ['ORG', 'PERSON', 'GPE', 'MONEY', 'PERCENT']:
                key_article_entities.add(ent.text.lower())

        embed_entity_mentions = sum(1 for entity in key_article_entities
                                  if entity in embed_lower)
        if embed_entity_mentions > 0:
            cross_reference_boost += min(0.15, embed_entity_mentions * 0.05)  # Cap at 0.15

        # 2. Authority language patterns (using semantic similarity to authoritative content)
        # Compare embed against patterns of authoritative communication
        authority_patterns = [
            "official statement",
            "according to sources",
            "confirmed by",
            "announced today",
            "press release",
            "spokesperson said"
        ]

        authority_embeddings = semantic_model.encode(authority_patterns, convert_to_tensor=True)
        embed_auth_similarity = torch.mean(torch.cosine_similarity(
            embed_embedding.unsqueeze(0), authority_embeddings, dim=1
        )).item()

        authority_boost = embed_auth_similarity * 0.1  # Scale down the authority signal

        # 3. Topic coherence: Are they discussing the same subject?
        # Use sentence-level analysis for better topic detection
        article_sentences = [sent.text.strip() for sent in article_doc.sents if len(sent.text.strip()) > 10]
        embed_sentences = [sent.text.strip() for sent in embed_doc.sents if len(sent.text.strip()) > 10]

        if article_sentences and embed_sentences:
            # Compare sentence embeddings for topic coherence
            article_sent_embeddings = semantic_model.encode(article_sentences[:3], convert_to_tensor=True)  # First 3 sentences
            embed_sent_embeddings = semantic_model.encode(embed_sentences[:2], convert_to_tensor=True)    # First 2 sentences

            topic_coherence = torch.mean(torch.cosine_similarity(
                article_sent_embeddings.unsqueeze(1),
                embed_sent_embeddings.unsqueeze(0),
                dim=2
            )).item()

            topic_boost = topic_coherence * 0.1
        else:
            topic_boost = 0

        # Combine intelligent boosts
        content_boost = cross_reference_boost + authority_boost + topic_boost

        # Enhanced scoring algorithm
        # Semantic similarity is now the primary factor (60%)
        # Entity overlap catches important names/places (25%)
        # Content boost uses AI to assess relevance without hardcoded keywords (15%)
        final_score = (semantic_sim * 0.6) + (entity_overlap * 0.25) + content_boost

        # Adaptive threshold based on content length
        if len(request.embedContent) < 50:  # Very short embeds
            threshold = 0.3
        elif len(request.embedContent) < 150:  # Medium embeds
            threshold = 0.25
        else:  # Long embeds
            threshold = 0.2

        keep = final_score > threshold

        # Generate detailed explanation
        explanation = generate_semantic_explanation(
            semantic_sim, entity_overlap, content_boost, final_score, threshold, keep,
            article_entities, embed_entities, request
        )

        return {
            "keep": keep,
            "final_score": round(final_score, 3),
            "threshold": round(threshold, 3),
            "scores": {
                "semantic_similarity": round(semantic_sim, 3),
                "entity_overlap": round(entity_overlap, 3),
                "content_boost": round(content_boost, 3)
            },
            "explanation": explanation
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

def generate_semantic_explanation(semantic_sim, entity_overlap, content_boost, final_score, threshold, keep,
                                article_entities, embed_entities, request):
    """Generate human-readable explanation of the semantic analysis results."""

    explanation = []

    # Overall decision
    if keep:
        explanation.append("‚úÖ **EMBED KEPT** - This embed appears relevant to the article.")
    else:
        explanation.append("‚ùå **EMBED REJECTED** - This embed does not appear relevant to the article.")

    explanation.append(f"**Overall Score**: {final_score:.3f}/1.000 (threshold: {threshold:.3f})")

    # Semantic Similarity Analysis
    explanation.append("\nüß† **Semantic Understanding Analysis**")
    if semantic_sim < 0.2:
        explanation.append(f"‚Ä¢ Very low semantic similarity ({semantic_sim:.3f}) - The AI model detects different topics and meanings.")
    elif semantic_sim < 0.4:
        explanation.append(f"‚Ä¢ Low semantic similarity ({semantic_sim:.3f}) - Some conceptual overlap but different main topics.")
    elif semantic_sim < 0.7:
        explanation.append(f"‚Ä¢ Good semantic similarity ({semantic_sim:.3f}) - Related concepts and contextual connections.")
    else:
        explanation.append(f"‚Ä¢ High semantic similarity ({semantic_sim:.3f}) - Strong semantic relationship between article and embed.")

    # Entity Analysis
    explanation.append("\nüè∑Ô∏è **Named Entity Analysis**")
    shared_entities = article_entities.intersection(embed_entities)
    if len(shared_entities) == 0:
        explanation.append("‚Ä¢ No shared named entities - Different people, organizations, or places mentioned.")
    else:
        explanation.append(f"‚Ä¢ Found {len(shared_entities)} shared entities: {', '.join(list(shared_entities)[:5])}{'...' if len(shared_entities) > 5 else ''}")
        explanation.append(f"‚Ä¢ Entity overlap: {entity_overlap:.3f} - {'Strong entity alignment' if entity_overlap > 0.3 else 'Some entity overlap' if entity_overlap > 0.1 else 'Limited entity alignment'}")

    # Content Quality Analysis
    explanation.append("\nüéØ **Content Quality Analysis**")
    if content_boost > 0.15:
        explanation.append(f"‚Ä¢ Strong content alignment (+{content_boost:.2f}) - Embed shows high relevance through cross-referencing and topic coherence.")
    elif content_boost > 0.05:
        explanation.append(f"‚Ä¢ Moderate content alignment (+{content_boost:.2f}) - Some relevant connections detected.")
    else:
        explanation.append("‚Ä¢ Limited content alignment detected.")

    # Specific insights
    explanation.append("\nüí° **AI Analysis Insights**")
    if not keep:
        if semantic_sim < 0.2:
            explanation.append("‚Ä¢ **Primary issue**: AI model identifies fundamentally different topics")
        if entity_overlap < 0.05:
            explanation.append("‚Ä¢ **Missing connection**: No shared people, organizations, or locations")
        if content_boost == 0:
            explanation.append("‚Ä¢ **Quality concern**: Limited content alignment with article")
    else:
        if semantic_sim > 0.5:
            explanation.append("‚Ä¢ **Strength**: Strong semantic understanding of related content")
        if entity_overlap > 0.2:
            explanation.append("‚Ä¢ **Strength**: Key entities are consistent between article and embed")
        if content_boost > 0.1:
            explanation.append("‚Ä¢ **Strength**: High content alignment and relevance")

    # Summary recommendation
    explanation.append(f"\nüìä **Final Assessment**: Score {final_score:.3f} vs threshold {threshold:.3f}")
    if keep:
        explanation.append("The AI model recommends **KEEPING** this embed as it adds value to the article content.")
    else:
        explanation.append("The AI model recommends **REJECTING** this embed as it lacks sufficient relevance.")

    return "\n".join(explanation)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)