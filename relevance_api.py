from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import spacy
from thefuzz import fuzz
import uvicorn
from typing import Optional, Dict
from sentence_transformers import SentenceTransformer
import torch
import numpy as np

app = FastAPI(title="Embed Relevance Analyzer", version="1.0.0")

# Load semantic model (multilingual)
print("Loading semantic model...")
semantic_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
print("Semantic model loaded!")

# Load spaCy model
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    # If model not found, download it
    import subprocess
    subprocess.run(["python", "-m", "spacy", "download", "en_core_web_sm"])
    nlp = spacy.load("en_core_web_sm")

class RelevanceRequest(BaseModel):
    articleTitle: str
    articleSummary: str
    embedType: str
    embedContent: str
    articleText: str
    # Optional configuration parameters
    threshold: Optional[float] = None
    strictness: Optional[str] = "standard"  # "strict", "standard", "lenient"
    domain: Optional[str] = "general"  # "news", "tech", "business", "medical", "legal"

class RelevanceResponse(BaseModel):
    keep: bool
    reason: str
    scores: Optional[dict] = None

@app.post("/analyze-relevance", response_model=RelevanceResponse)
async def analyze_relevance(request: RelevanceRequest):
    """
    Analyze if an embed is relevant to an article using semantic understanding.
    """
    try:
        # Create comprehensive article text
        article_text = f"{request.articleTitle}. {request.articleSummary}. {request.articleText}"

        # Semantic similarity using sentence transformers (multilingual)
        article_embedding = semantic_model.encode(article_text, convert_to_tensor=True)
        embed_embedding = semantic_model.encode(request.embedContent, convert_to_tensor=True)

        # Calculate cosine similarity between embeddings
        semantic_sim = torch.cosine_similarity(article_embedding, embed_embedding, dim=0).item()

        # Traditional TF-IDF as backup (for very short texts)
        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
        texts = [article_text, request.embedContent]
        vectors = vectorizer.fit_transform(texts)
        tfidf_sim = float(cosine_similarity(vectors[0:1], vectors[1:2])[0][0])

        # Named entity overlap (important for news articles)
        article_doc = nlp(article_text)
        embed_doc = nlp(request.embedContent)

        article_entities = set([ent.text.lower() for ent in article_doc.ents])
        embed_entities = set([ent.text.lower() for ent in embed_doc.ents])

        entity_overlap = len(article_entities.intersection(embed_entities)) / len(article_entities.union(embed_entities)) if article_entities.union(embed_entities) else 0

        # Smart content-based relevance scoring (no hardcoded keywords)
        # Analyze how well the embed content aligns with and references the article

        # 1. Cross-reference boost: Does the embed appear to reference the article?
        cross_reference_boost = 0

        embed_lower = request.embedContent.lower()

        # Check if embed mentions key entities from the article
        key_article_entities = set()
        for ent in article_doc.ents:
            if ent.label_ in ['ORG', 'PERSON', 'GPE', 'MONEY', 'PERCENT', 'EVENT', 'PRODUCT', 'WORK_OF_ART', 'LAW', 'DATE', 'FAC', 'LOC']:
                key_article_entities.add(ent.text.lower())

        embed_entity_mentions = sum(1 for entity in key_article_entities
                                  if entity in embed_lower)
        if embed_entity_mentions > 0:
            cross_reference_boost += min(0.15, embed_entity_mentions * 0.05)  # Cap at 0.15

        # 2. Authority language patterns (using semantic similarity to authoritative content)
        # Compare embed against patterns of authoritative communication
        authority_patterns = [
            # News & Reporting
            "breaking news",
            "just in",
            "we can confirm",
            "sources say",
            "eyewitnesses report",
            "latest development",
            "updates indicate",

            # Official Announcements
            "the company announced",
            "government stated",
            "official data shows",
            "we are pleased to announce",
            "board approved",
            "quarterly results",
            "financial report",

            # Expert Attribution
            "experts believe",
            "analysts say",
            "researchers found",
            "study reveals",
            "scientific evidence suggests",
            "medical experts warn",
            "economic forecast",

            # Direct Quotes & Statements
            "told reporters",
            "said in a statement",
            "quoted as saying",
            "declared that",
            "testified that",
            "commented on",
            "spokesperson confirmed",

            # Time-sensitive Language
            "earlier today",
            "this morning",
            "just hours ago",
            "recently revealed",
            "latest information",
            "real-time updates",

            # Verification & Credibility
            "has confirmed",
            "verified by",
            "authentic sources",
            "reliable information",
            "cross-check shows",
            "fact-check confirmed",

            # Legal & Regulatory
            "court ruled",
            "legislation passed",
            "regulatory approval",
            "compliance verified",
            "legal document states",
            "judgment issued",

            # Technical & Scientific
            "peer-reviewed",
            "clinical trial",
            "data analysis shows",
            "research indicates",
            "experimental results",
            "technical specifications",

            # Corporate/Business
            "executive decision",
            "market analysis",
            "industry report",
            "strategic initiative",
            "operational update",
            "business intelligence",

            # Emergency/Official Alerts
            "emergency announcement",
            "public safety",
            "weather warning",
            "health advisory",
            "security alert",

            # Multilingual equivalents (for better cross-language support)
            "d√©claration officielle",
            "comunicado oficial",
            "offizielle erkl√§rung",
            "ÂÖ¨ÂºèÂ£∞Êòé",
            "Í≥µÏãù ÏÑ±Î™Ö",
            "ufficial dichiarazione"
        ]

        authority_embeddings = semantic_model.encode(authority_patterns, convert_to_tensor=True)
        embed_auth_similarity = torch.mean(torch.cosine_similarity(
            embed_embedding.unsqueeze(0), authority_embeddings, dim=1
        )).item()

        authority_boost = embed_auth_similarity * 0.1  # Scale down the authority signal

        # 3. Topic coherence: Are they discussing the same subject?
        # Use sentence-level analysis for better topic detection
        article_sentences = [sent.text.strip() for sent in article_doc.sents if len(sent.text.strip()) > 10]
        embed_sentences = [sent.text.strip() for sent in embed_doc.sents if len(sent.text.strip()) > 10]

        if article_sentences and embed_sentences:
            # Compare sentence embeddings for topic coherence
            article_sent_embeddings = semantic_model.encode(article_sentences[:3], convert_to_tensor=True)  # First 3 sentences
            embed_sent_embeddings = semantic_model.encode(embed_sentences[:2], convert_to_tensor=True)    # First 2 sentences

            topic_coherence = torch.mean(torch.cosine_similarity(
                article_sent_embeddings.unsqueeze(1),
                embed_sent_embeddings.unsqueeze(0),
                dim=2
            )).item()

            topic_boost = topic_coherence * 0.1
        else:
            topic_boost = 0

        # Combine intelligent boosts
        content_boost = cross_reference_boost + authority_boost + topic_boost

        # Enhanced scoring algorithm
        # Semantic similarity is now the primary factor (60%)
        # Entity overlap catches important names/places (25%)
        # Content boost uses AI to assess relevance without hardcoded keywords (15%)
        final_score = (semantic_sim * 0.6) + (entity_overlap * 0.25) + content_boost

        # Use custom threshold or calculate adaptive threshold
        if request.threshold is not None:
            threshold = request.threshold
        else:
            # Adaptive threshold based on content length and strictness
            if len(request.embedContent) < 50:  # Very short embeds
                base_threshold = 0.3
            elif len(request.embedContent) < 150:  # Medium embeds
                base_threshold = 0.25
            else:  # Long embeds
                base_threshold = 0.2

            # Adjust based on strictness
            if request.strictness == "strict":
                threshold = base_threshold + 0.1
            elif request.strictness == "lenient":
                threshold = base_threshold - 0.1
            else:  # standard
                threshold = base_threshold

        keep = final_score > threshold

        # Generate meaningful reason
        if semantic_sim > 0.6:
            semantic_reason = "Strong semantic similarity"
        elif semantic_sim > 0.3:
            semantic_reason = "Good semantic similarity"
        else:
            semantic_reason = "Low semantic similarity"

        if entity_overlap > 0.3:
            entity_reason = "high entity overlap"
        elif entity_overlap > 0.1:
            entity_reason = "some entity overlap"
        else:
            entity_reason = "low entity overlap"

        boost_reason = f"content boost: {content_boost:.2f}" if content_boost > 0 else "no content boost"

        reason = f"{semantic_reason}, {entity_reason}, {boost_reason}"

        scores = {
            "semantic_similarity": round(semantic_sim, 3),
            "entity_overlap": round(entity_overlap, 3),
            "content_boost": round(content_boost, 3),
            "final_score": round(final_score, 3),
            "threshold": round(threshold, 3)
        }

        return RelevanceResponse(keep=keep, reason=reason, scores=scores)

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy"}

@app.get("/config")
async def get_config_info():
    """Get information about available configuration options"""
    return {
        "strictness_levels": {
            "strict": "Higher threshold, more selective filtering (+0.1 to base threshold)",
            "standard": "Default balanced approach (base threshold)",
            "lenient": "Lower threshold, more permissive filtering (-0.1 to base threshold)"
        },
        "domains": {
            "general": "General purpose relevance detection",
            "news": "Optimized for news articles and journalistic content",
            "tech": "Optimized for technical and scientific content",
            "business": "Optimized for business and corporate content",
            "medical": "Optimized for medical and health content",
            "legal": "Optimized for legal and regulatory content"
        },
        "threshold_info": {
            "description": "Custom threshold value (0.0-1.0). If provided, overrides adaptive calculation",
            "recommended_ranges": {
                "high_precision": "0.3-0.4 (fewer false positives)",
                "balanced": "0.2-0.3 (good balance)",
                "high_recall": "0.1-0.2 (fewer false negatives)"
            }
        },
        "improvements": [
            "Enhanced entity recognition (13 types vs 5 types)",
            "Comprehensive authority patterns (80+ patterns)",
            "Multilingual support for authority detection",
            "Configurable thresholds and strictness levels",
            "Domain-specific optimization",
            "Semantic similarity instead of keyword matching"
        ]
    }

@app.post("/explain-relevance")
async def explain_relevance(request: RelevanceRequest):
    """
    Provide detailed explanation of embed relevance analysis using semantic understanding.
    """
    try:
        # Create comprehensive article text
        article_text = f"{request.articleTitle}. {request.articleSummary}. {request.articleText}"

        # Semantic similarity using sentence transformers (multilingual)
        article_embedding = semantic_model.encode(article_text, convert_to_tensor=True)
        embed_embedding = semantic_model.encode(request.embedContent, convert_to_tensor=True)

        # Calculate cosine similarity between embeddings
        semantic_sim = torch.cosine_similarity(article_embedding, embed_embedding, dim=0).item()

        # Named entity overlap (important for news articles)
        article_doc = nlp(article_text)
        embed_doc = nlp(request.embedContent)

        article_entities = set([ent.text.lower() for ent in article_doc.ents])
        embed_entities = set([ent.text.lower() for ent in embed_doc.ents])

        entity_overlap = len(article_entities.intersection(embed_entities)) / len(article_entities.union(embed_entities)) if article_entities.union(embed_entities) else 0

        # Smart content-based relevance scoring (no hardcoded keywords)
        # Analyze how well the embed content aligns with and references the article

        # 1. Cross-reference boost: Does the embed appear to reference the article?
        cross_reference_boost = 0

        embed_lower = request.embedContent.lower()

        # Check if embed mentions key entities from the article
        key_article_entities = set()
        for ent in article_doc.ents:
            if ent.label_ in ['ORG', 'PERSON', 'GPE', 'MONEY', 'PERCENT', 'EVENT', 'PRODUCT', 'WORK_OF_ART', 'LAW', 'DATE', 'FAC', 'LOC']:
                key_article_entities.add(ent.text.lower())

        embed_entity_mentions = sum(1 for entity in key_article_entities
                                  if entity in embed_lower)
        if embed_entity_mentions > 0:
            cross_reference_boost += min(0.15, embed_entity_mentions * 0.05)  # Cap at 0.15

        # 2. Authority language patterns (using semantic similarity to authoritative content)
        # Compare embed against patterns of authoritative communication
        authority_patterns = [
            # News & Reporting
            "breaking news",
            "just in",
            "we can confirm",
            "sources say",
            "eyewitnesses report",
            "latest development",
            "updates indicate",

            # Official Announcements
            "the company announced",
            "government stated",
            "official data shows",
            "we are pleased to announce",
            "board approved",
            "quarterly results",
            "financial report",

            # Expert Attribution
            "experts believe",
            "analysts say",
            "researchers found",
            "study reveals",
            "scientific evidence suggests",
            "medical experts warn",
            "economic forecast",

            # Direct Quotes & Statements
            "told reporters",
            "said in a statement",
            "quoted as saying",
            "declared that",
            "testified that",
            "commented on",
            "spokesperson confirmed",

            # Time-sensitive Language
            "earlier today",
            "this morning",
            "just hours ago",
            "recently revealed",
            "latest information",
            "real-time updates",

            # Verification & Credibility
            "has confirmed",
            "verified by",
            "authentic sources",
            "reliable information",
            "cross-check shows",
            "fact-check confirmed",

            # Legal & Regulatory
            "court ruled",
            "legislation passed",
            "regulatory approval",
            "compliance verified",
            "legal document states",
            "judgment issued",

            # Technical & Scientific
            "peer-reviewed",
            "clinical trial",
            "data analysis shows",
            "research indicates",
            "experimental results",
            "technical specifications",

            # Corporate/Business
            "executive decision",
            "market analysis",
            "industry report",
            "strategic initiative",
            "operational update",
            "business intelligence",

            # Emergency/Official Alerts
            "emergency announcement",
            "public safety",
            "weather warning",
            "health advisory",
            "security alert",

            # Multilingual equivalents (for better cross-language support)
            "d√©claration officielle",
            "comunicado oficial",
            "offizielle erkl√§rung",
            "ÂÖ¨ÂºèÂ£∞Êòé",
            "Í≥µÏãù ÏÑ±Î™Ö",
            "ufficial dichiarazione"
        ]

        authority_embeddings = semantic_model.encode(authority_patterns, convert_to_tensor=True)
        embed_auth_similarity = torch.mean(torch.cosine_similarity(
            embed_embedding.unsqueeze(0), authority_embeddings, dim=1
        )).item()

        authority_boost = embed_auth_similarity * 0.1  # Scale down the authority signal

        # 3. Topic coherence: Are they discussing the same subject?
        # Use sentence-level analysis for better topic detection
        article_sentences = [sent.text.strip() for sent in article_doc.sents if len(sent.text.strip()) > 10]
        embed_sentences = [sent.text.strip() for sent in embed_doc.sents if len(sent.text.strip()) > 10]

        if article_sentences and embed_sentences:
            # Compare sentence embeddings for topic coherence
            article_sent_embeddings = semantic_model.encode(article_sentences[:3], convert_to_tensor=True)  # First 3 sentences
            embed_sent_embeddings = semantic_model.encode(embed_sentences[:2], convert_to_tensor=True)    # First 2 sentences

            topic_coherence = torch.mean(torch.cosine_similarity(
                article_sent_embeddings.unsqueeze(1),
                embed_sent_embeddings.unsqueeze(0),
                dim=2
            )).item()

            topic_boost = topic_coherence * 0.1
        else:
            topic_boost = 0

        # Combine intelligent boosts
        content_boost = cross_reference_boost + authority_boost + topic_boost

        # Enhanced scoring algorithm
        # Semantic similarity is now the primary factor (60%)
        # Entity overlap catches important names/places (25%)
        # Content boost uses AI to assess relevance without hardcoded keywords (15%)
        final_score = (semantic_sim * 0.6) + (entity_overlap * 0.25) + content_boost

        # Use custom threshold or calculate adaptive threshold
        if request.threshold is not None:
            threshold = request.threshold
        else:
            # Adaptive threshold based on content length and strictness
            if len(request.embedContent) < 50:  # Very short embeds
                base_threshold = 0.3
            elif len(request.embedContent) < 150:  # Medium embeds
                base_threshold = 0.25
            else:  # Long embeds
                base_threshold = 0.2

            # Adjust based on strictness
            if request.strictness == "strict":
                threshold = base_threshold + 0.1
            elif request.strictness == "lenient":
                threshold = base_threshold - 0.1
            else:  # standard
                threshold = base_threshold

        keep = final_score > threshold

        # Generate detailed explanation
        explanation = generate_semantic_explanation(
            semantic_sim, entity_overlap, content_boost, final_score, threshold, keep,
            article_entities, embed_entities
        )

        return {
            "keep": keep,
            "final_score": round(final_score, 3),
            "threshold": round(threshold, 3),
            "scores": {
                "semantic_similarity": round(semantic_sim, 3),
                "entity_overlap": round(entity_overlap, 3),
                "content_boost": round(content_boost, 3)
            },
            "explanation": explanation
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

def generate_semantic_explanation(semantic_sim, entity_overlap, content_boost, final_score, threshold, keep,
                                article_entities, embed_entities):
    """Generate human-readable explanation of the semantic analysis results."""

    explanation = []

    # Overall decision
    if keep:
        explanation.append("‚úÖ **EMBED KEPT** - This embed appears relevant to the article.")
    else:
        explanation.append("‚ùå **EMBED REJECTED** - This embed does not appear relevant to the article.")

    explanation.append(f"**Overall Score**: {final_score:.3f}/1.000 (threshold: {threshold:.3f})")

    # Semantic Similarity Analysis
    explanation.append("\nüß† **Semantic Understanding Analysis**")
    if semantic_sim < 0.2:
        explanation.append(f"‚Ä¢ Very low semantic similarity ({semantic_sim:.3f}) - The AI model detects different topics and meanings.")
    elif semantic_sim < 0.4:
        explanation.append(f"‚Ä¢ Low semantic similarity ({semantic_sim:.3f}) - Some conceptual overlap but different main topics.")
    elif semantic_sim < 0.7:
        explanation.append(f"‚Ä¢ Good semantic similarity ({semantic_sim:.3f}) - Related concepts and contextual connections.")
    else:
        explanation.append(f"‚Ä¢ High semantic similarity ({semantic_sim:.3f}) - Strong semantic relationship between article and embed.")

    # Entity Analysis
    explanation.append("\nüè∑Ô∏è **Named Entity Analysis**")
    shared_entities = article_entities.intersection(embed_entities)
    if len(shared_entities) == 0:
        explanation.append("‚Ä¢ No shared named entities - Different people, organizations, or places mentioned.")
    else:
        explanation.append(f"‚Ä¢ Found {len(shared_entities)} shared entities: {', '.join(list(shared_entities)[:5])}{'...' if len(shared_entities) > 5 else ''}")
        explanation.append(f"‚Ä¢ Entity overlap: {entity_overlap:.3f} - {'Strong entity alignment' if entity_overlap > 0.3 else 'Some entity overlap' if entity_overlap > 0.1 else 'Limited entity alignment'}")

    # Content Quality Analysis
    explanation.append("\nüéØ **Content Quality Analysis**")
    if content_boost > 0.15:
        explanation.append(f"‚Ä¢ Strong content alignment (+{content_boost:.2f}) - Embed shows high relevance through cross-referencing and topic coherence.")
    elif content_boost > 0.05:
        explanation.append(f"‚Ä¢ Moderate content alignment (+{content_boost:.2f}) - Some relevant connections detected.")
    else:
        explanation.append("‚Ä¢ Limited content alignment detected.")

    # Specific insights
    explanation.append("\nüí° **AI Analysis Insights**")
    if not keep:
        if semantic_sim < 0.2:
            explanation.append("‚Ä¢ **Primary issue**: AI model identifies fundamentally different topics")
        if entity_overlap < 0.05:
            explanation.append("‚Ä¢ **Missing connection**: No shared people, organizations, or locations")
        if content_boost == 0:
            explanation.append("‚Ä¢ **Quality concern**: Limited content alignment with article")
    else:
        if semantic_sim > 0.5:
            explanation.append("‚Ä¢ **Strength**: Strong semantic understanding of related content")
        if entity_overlap > 0.2:
            explanation.append("‚Ä¢ **Strength**: Key entities are consistent between article and embed")
        if content_boost > 0.1:
            explanation.append("‚Ä¢ **Strength**: High content alignment and relevance")

    # Summary recommendation
    explanation.append(f"\nüìä **Final Assessment**: Score {final_score:.3f} vs threshold {threshold:.3f}")
    if keep:
        explanation.append("The AI model recommends **KEEPING** this embed as it adds value to the article content.")
    else:
        explanation.append("The AI model recommends **REJECTING** this embed as it lacks sufficient relevance.")

    return "\n".join(explanation)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)